# Kinnect - Viability Analysis V2 (Rubric-Based)

**Project**: Kinnect - Crowdsourced Fitness Platform
**Authors**: Emily Lo (emlo), Caroline Cummings (carfc), Alex Oh (alexoh), Shivi Jain (shivij)
**Analysis Date**: 2025-11-02

---

## Viability Score Breakdown

| Dimension | Score | Weight | Reasoning |
|-----------|-------|--------|-----------|
| Technical Feasibility | 1/5 | Critical | Mobile + 5 APIs + real-time + ML = impossible in 5 weeks |
| Crowdsourcing Viability | 4/5 | High | Strong intrinsic motivation model, but needs critical mass |
| Incentive Design | 5/5 | High | Excellent alignment of effort/reward with gamification |
| Scope Appropriateness | 1/5 | Critical | 10 features, multiple subsystems = 3x overscoped |
| Recruitment Strategy | 3/5 | Medium | Local pilot strategy solid, but 500+ target unrealistic |
| Quality Control | 4/5 | Medium | Sophisticated multi-layer approach well-designed |

**TOTAL SCORE: 18/30**

**VERDICT: NEEDS MAJOR REVISION**

---

## Executive Summary

Kinnect proposes a compelling crowdsourced fitness platform with excellent incentive design and sophisticated quality control thinking, but suffers from catastrophic technical overscoping. The team is attempting to build React Native mobile apps, integrate 5+ external APIs (HealthKit, Fitbit, Strava, Mapbox, Stripe), implement real-time data pipelines with Python ML for fraud detection, and create a rewards/sponsorship ecosystem‚Äîthis represents 6+ months of professional development work compressed into 5 weeks with zero budget. While the crowdsourcing principles are sound and the local "Penn Challenge" pilot strategy is smart, the project requires immediate 70% descoping to web-only with manual logging to be viable.

---

## Detailed Assessment

### 1. Technical Feasibility: 1/5 ‚ö†Ô∏è

**Score Justification**: Multiple impossible technical requirements stacked together.

**Specific Strengths**:
- Team demonstrates strong technical knowledge in proposal architecture
- Smart choice of modern stack (React Native, Firebase, MongoDB)
- Thoughtful separation of concerns (Firebase Functions vs AWS Lambdas)
- Clear understanding of data flow and aggregation needs

**Specific Weaknesses**:
- **Mobile development alone = 3-4 weeks**: React Native requires iOS/Android setup, testing, deployment
- **API integration nightmare**: HealthKit/Fitbit/Strava require developer approval (2-4 weeks), API key management, OAuth flows, data normalization
- **Mapbox real-time visualization**: Complex geographic aggregation and real-time updates require sophisticated backend
- **Dual backend architecture**: Firebase Functions AND AWS Lambdas adds unnecessary complexity
- **Python ML for fraud detection**: Building, training, and deploying outlier detection models is a project unto itself
- **Stripe integration**: Payment processing adds legal/compliance overhead
- **Real-time features**: "Live" map and leaderboards require WebSockets or polling infrastructure

**Technical Debt Analysis**:
```
React Native mobile app:        3 weeks
5 API integrations:              2 weeks (optimistic, assumes instant approval)
Real-time Mapbox visualization:  1.5 weeks
Python ML fraud detection:       1 week
Firebase + MongoDB + AWS:        1 week setup/integration
Stripe rewards system:           1 week
TOTAL:                          9.5 weeks for skilled team
```

**Concrete Recommendations**:
1. **Eliminate mobile entirely**: Build responsive web app (React/Next.js) - saves 3 weeks
2. **Cut all API integrations for MVP**: Manual activity logging only - saves 2 weeks
3. **Replace real-time map with static leaderboard**: Simple sorted list - saves 1.5 weeks
4. **Cut ML fraud detection**: Basic threshold rules (e.g., max 20 miles/day) - saves 1 week
5. **Cut Stripe/rewards**: Display points only, no redemption - saves 1 week
6. **Single backend**: Firebase only OR simple Node.js + MongoDB - saves setup time

**Descoped MVP**:
- Web app (React)
- Firebase Auth
- MongoDB for user/activity storage
- Manual activity logging form
- Simple team leaderboards (no map)
- Basic point calculation (no ML)
- Static weekly challenges

---

### 2. Crowdsourcing Viability: 4/5 ‚úÖ

**Score Justification**: Strong intrinsic motivation model with self-reinforcing loop, but network effects create chicken-egg risk.

**Specific Strengths**:
- **Excellent incentive alignment**: Users receive value (motivation, accountability) by providing value (activity data)
- **Low barrier to entry**: No special skills needed - just be physically active
- **Smart local pilot strategy**: "Penn Challenge" focuses initial users, making leaderboards feel alive from day one
- **Natural viral loop**: Team challenges incentivize friend recruitment
- **Inclusive activity model**: Walking to marathon running all count - democratizes participation
- **Strong reference model**: Explicitly modeled on Duolingo's proven habit-building mechanics

**Specific Weaknesses**:
- **Critical mass problem**: Proposal states "500+ users for full functionality" - this is unrealistic for class project timeline
- **Network effects dependency**: Value proposition weakens with low user count (empty leaderboards, no global map excitement)
- **Cold start vulnerability**: Despite "bot team" backup plan, new users joining sparse leaderboards likely to churn immediately
- **Team formation friction**: Requires existing friend groups or random assignment (which reduces social accountability)

**Concrete Recommendations**:
1. **Redefine "full functionality"**: Design for 30-50 users as primary experience, not 500+
2. **Pre-commit users**: Get 20-30 Penn students to commit BEFORE building (GroupMe, class signup)
3. **Single-team pilot**: Launch as one cohesive "Penn Challenge" team competing against historical data or team-based goals, not multi-team competition
4. **Seed with team data**: Developers log real activities for 1-2 weeks pre-launch to create baseline activity
5. **Add solo mode value**: Ensure app provides value even without active teams (personal streaks, goals, achievements)

**Risk Mitigation**:
- Week 1: Get 30 confirmed users or pivot immediately
- If <10 active users by Week 2: Switch from competitive leaderboards to collaborative goals ("Can Penn hit 500 miles this week?")

---

### 3. Incentive Design: 5/5 ‚úÖ

**Score Justification**: Textbook example of well-matched incentive structure.

**Specific Strengths**:
- **Perfect effort/incentive ratio**: Task is <2 minutes (log workout) + intrinsically fun (exercise) = gamification works well
- **Hybrid motivation model**: Primary intrinsic (health, social accountability) with secondary extrinsic (points, badges) is research-backed
- **Social accountability**: "Not letting team down" is powerful motivator, especially with real friends
- **FOMO mechanics**: Streaks and team challenges create fear of missing out
- **Progression systems**: Unlockable badges and point accumulation provide sense of advancement
- **Weighted contributions**: Smart recognition that verified intense workouts should count more than self-reported walks
- **Anti-overexercise design**: Daily point caps prevent unhealthy behavior - shows ethical thinking

**Specific Weaknesses**:
- **Rewards system is business fantasy**: Proposal assumes partnerships with fitness brands, local gyms, charitable organizations - none of which exist yet
- **Extrinsic rewards unsustainable**: Once points can't be redeemed for real rewards, motivation may drop
- **Assumes sustained team participation**: If teammates quit, social accountability collapses

**Concrete Recommendations**:
1. **Cut real rewards for MVP**: Points displayed but not redeemable - sets expectations correctly
2. **Focus on intrinsic only**: Emphasize personal health goals, team camaraderie, achievement showcase
3. **Add showcase value**: Public profiles with achievement timelines - let users show off their consistency
4. **Leverage Penn context**: "Help Penn rank #1 in Ivy League activity" creates institutional pride motivation
5. **Weekly micro-challenges**: "Log 5 activities this week for special badge" - short-term achievable goals maintain momentum

**Why This Works**:
The core task (exercising and logging it) is something users WANT to do anyway. The platform simply adds structure, accountability, and fun. This is the "Waze model" - users contribute data as natural byproduct of using the service, which is ideal crowdsourcing design.

---

### 4. Scope Appropriateness: 1/5 ‚ö†Ô∏è

**Score Justification**: Attempting to build a venture-backed startup in 5 weeks.

**Specific Strengths**:
- Team has identified core features (logging, teams, leaderboards)
- Proposal shows understanding of MVP concept with "minimum viable crowd" of 20-30 users
- Clear differentiation between essential and nice-to-have in some areas

**Specific Weaknesses**:
- **10 key features listed**: Live global map, daily logging, API sync, streaks, team challenges, leaderboards, points/rewards, social feed, user profiles, charitable donations
- **Each feature is a sub-project**:
  - Live global Mapbox visualization = 1 week
  - API integrations (4 services) = 2 weeks
  - Social feed = 1 week
  - Rewards redemption system = 1 week
  - User profiles with badges = 3-4 days
- **Multiple subsystems**: Authentication, data ingestion, aggregation pipelines, real-time updates, QC fraud detection, rewards processing
- **"Potential to scale to 1,000,000+ users"**: Proposal conflates class project with business vision

**Scope Reality Check**:
| Question | Answer | Problem? |
|----------|--------|----------|
| Can you build core loop in 2 weeks? | No - requires mobile + APIs | ‚ö†Ô∏è YES |
| Can you manually simulate system? | Partially - can log activities manually, but real-time map can't be simulated | ‚ö†Ô∏è YES |
| Can you name exactly 3-4 must-haves? | No - 10 features listed, most marked as essential | ‚ö†Ô∏è YES |

**Concrete Recommendations**:
1. **Apply "Must/Should/Could/Won't" framework**:
   - **MUST**: User signup, manual activity logging, team assignment, simple leaderboard (sorted list)
   - **SHOULD**: Personal streaks, basic point calculation
   - **COULD**: Activity verification, badges, social feed
   - **WON'T**: Real-time map, API integrations, rewards redemption, ML fraud detection, charitable donations

2. **Build horizontal slice, not vertical**: Complete end-to-end for ONE workflow (log workout ‚Üí see updated leaderboard) before adding features

3. **Two-week deliverable test**: "By Week 2, we'll have a web app where Penn students can create accounts, log daily workouts manually, join 'Team Penn', and see a leaderboard sorted by total weekly points." If this isn't achievable, scope is too large.

4. **Feature freeze after Week 3**: Weeks 4-5 are for polish, testing, recruiting users, fixing bugs - NOT building new features

**Realistic 5-Week Scope**:
- Week 1: Auth + basic database + manual logging form
- Week 2: Team assignment + leaderboard display + point calculation
- Week 3: Personal streaks + activity feed + basic QC (max thresholds)
- Week 4: User testing + bug fixes + recruitment
- Week 5: Final polish + demo preparation

---

### 5. Recruitment Strategy: 3/5 üü°

**Score Justification**: Has specific local strategy but unrealistic scale expectations.

**Specific Strengths**:
- **Named specific channels**: Penn clubs, sports teams, student organizations
- **Smart local density strategy**: "Penn Challenge" focuses users geographically for better network effects
- **Viral loop designed in**: Team formation incentivizes friend recruitment
- **Multiple recruitment vectors**: Friends/family, social media, word-of-mouth
- **Realistic minimum viable crowd**: 20-30 users is achievable

**Specific Weaknesses**:
- **Target of 500+ users**: Massively unrealistic for 5-week project timeline
- **"Organic" recruitment vague**: Social media outreach to "student/fitness-focused" accounts lacks specificity
- **No commitments obtained**: Proposal doesn't indicate any users have been contacted or committed
- **Assumes "viral loop" works**: History shows viral loops rarely activate in class projects without existing traction
- **No backup recruitment plan**: If organic fails, no mention of paid recruitment (MTurk) or other alternatives

**Specificity Test**:
| Question | Answer | Pass? |
|----------|--------|-------|
| Can you NAME exact groups/places? | "Penn clubs, sports teams" - somewhat specific | üü° Partial |
| Have you TESTED recruitment? | No indication of outreach yet | ‚ùå NO |
| Do you need >50 people to start? | Proposal says 20-30 minimum | ‚úÖ YES |
| Can you seed it yourself? | Yes - team can log activities pre-launch | ‚úÖ YES |

**Concrete Recommendations**:
1. **Week 0 (NOW) recruitment test**:
   - Post in 3 specific Penn-related channels (class GroupChats, dorm Discords, club Slacks)
   - Message: "Would you join a 4-week Penn fitness challenge with leaderboards? React üëç if interested"
   - If <20 reactions in 48 hours ‚Üí pivot or add MTurk backup

2. **Get pre-commitments**:
   - Create Google Form: "Sign up for Penn Fitness Challenge - Starts [Date]"
   - Target: 30 signups before building anything
   - If you can't get 30 signups with a form, you won't get 30 active users with an app

3. **Leverage existing groups**:
   - Partner with ONE specific club (e.g., Penn Running Club, intramural team)
   - Get captain/leader buy-in to recruit their members
   - This provides captive audience + natural team structure

4. **Reduce target to reality**:
   - MVP target: 30 users (enough for 3 teams of 10)
   - Success target: 50 active users over 4 weeks
   - Stretch target: 100 users by end of semester
   - Drop "500+ for full functionality" - redesign around 30-50 user experience

5. **Team seeding strategy**:
   - Developers + friends join as "Team Alpha" Week 1
   - Log activities for 1 week to create baseline leaderboard data
   - This ensures new users see active leaderboard immediately (solves cold start)

6. **Backup plan**:
   - Budget $50 for MTurk if organic recruitment fails by Week 2
   - Task: "Log 5 workouts over 2 weeks, get $5" (10 workers = 50 activities)

---

### 6. Quality Control: 4/5 ‚úÖ

**Score Justification**: Sophisticated multi-layered approach with specific mechanisms, though some may be over-engineered for MVP.

**Specific Strengths**:
- **Multi-layered strategy**: API verification + statistical outlier detection + reputation scoring
- **Primary verification via APIs**: Auto-verified data from HealthKit/Fitbit is gold standard
- **Statistical outlier detection with specifics**: "3+ standard deviations from human possibility" with concrete examples (5-min marathon)
- **Reputation system design**: Trust Score that increases with verified data, decreases with anomalies
- **Progressive enforcement**: Low-quality data rejected ‚Üí repeat offenders penalized ‚Üí extreme abuse leads to suspension
- **Ethical consideration**: Daily point caps prevent over-exercising (QC for user safety, not just data quality)
- **Weighting system**: Verified data weighted 1.5x, self-reported 0.8x - incentivizes quality

**Specific Weaknesses**:
- **API verification not available in MVP**: If cutting API integrations (recommended), primary QC mechanism disappears
- **ML outlier detection overengineered**: "Python scripts with scikit-learn" for detecting impossible data is overkill when simple rules would work
- **Reputation system adds complexity**: Tracking trust scores, applying dynamic weights requires sophisticated backend
- **No redundancy**: Unlike many crowdsourcing systems, no multiple-worker validation (makes sense for personal data, but limits QC options)
- **Cold start for reputation**: New users have no trust score - how is initial data weighted?

**Concrete Recommendations**:
1. **MVP QC (without APIs/ML)**:
   - **Hard limits**: Max 50 miles/day running, 100 miles/day cycling, 50,000 steps/day walking
   - **Time limits**: Max 6 hours of activity per day (prevents spam)
   - **Manual spot checks**: Review top 10 leaderboard entries weekly for anomalies
   - **Community reporting**: "Flag suspicious activity" button with threshold (3 flags = review)

2. **Simple reputation system**:
   - Start everyone at "Neutral" trust level
   - Each flagged/rejected submission = -1 point
   - Each week of consistent logging = +1 point
   - If trust score < -3, restrict to 50% point value until 2 clean weeks

3. **Progressive QC rollout**:
   - **Week 1-2**: Manual review of all activities (low volume)
   - **Week 3**: Implement hard limit rules
   - **Week 4+**: Add community flagging if needed
   - **Post-project**: Could add API verification for real deployment

4. **Gold standard approach**:
   - Developers log activities with Strava screenshots as proof
   - Use this as baseline to calibrate what "normal" activity looks like
   - Compare new user submissions against this distribution

5. **Transparency**:
   - Show "Verified" vs "Self-Reported" badges on activities
   - Leaderboards can filter to "Verified Only" mode
   - This social pressure encourages honest reporting

**Why 4/5 Despite Concerns**:
The thinking is sophisticated and shows deep understanding of crowdsourcing QC principles. The proposal correctly identifies the core challenge (fraud/exaggeration) and designs appropriate mechanisms. The score is only docked because implementation as described is overcomplex for 5-week timeline - but the conceptual framework is excellent.

---

## Risk Analysis

### Risk 1: Technical Impossibility ‚ö†Ô∏è CRITICAL
**Likelihood**: 95%
**Impact**: Project failure

**Description**: Current technical scope (mobile + 5 APIs + real-time + ML) cannot be completed in 5 weeks. Team will spend Weeks 1-2 on React Native setup and API integration attempts, realize timeline is impossible Week 3, panic-pivot to web Week 4, deliver half-working demo Week 5.

**Mitigation Strategies**:
- **Immediate descope (TODAY)**: Cut to web-only + manual logging BEFORE writing any code
- **Build vertical slice Week 1**: End-to-end workflow (signup ‚Üí log ‚Üí see leaderboard) as proof-of-concept
- **No new features after Week 3**: Feature freeze, polish/recruitment only
- **Weekly reality checks**: "Could we demo this to a user today?" If no ‚Üí cut features
- **Backup architecture**: If Firebase is too complex, fall back to simple Node.js + MongoDB + static HTML/CSS

**If Mitigation Fails**: Project becomes classic overscoped disaster - impressive slide deck, minimal working demo, grade capped at B.

---

### Risk 2: Recruitment Failure (Cold Start) üü° HIGH
**Likelihood**: 60%
**Impact**: No users = no crowdsourcing = failed project

**Description**: Team builds platform but can't recruit 20-30 active users. Leaderboards stay empty, no team competition materializes, network effects never activate. Without users, can't demonstrate crowdsourcing principles or evaluate success metrics.

**Mitigation Strategies**:
- **Pre-commit users Week 0**: Get 30 signups via Google Form BEFORE building anything
- **Leverage existing community**: Partner with specific Penn club (running team, intramural group) for captive audience
- **Team pre-seeds data**: Developers log 1 week of activities before launch to create baseline
- **Lower activation threshold**: Redesign for 20 users as "full experience", not 500
- **Backup paid recruitment**: Budget $50 MTurk if organic fails ("Log 5 workouts in 2 weeks for $5" = 10 workers)
- **Pivot to collaborative if competitive fails**: If can't get teams, switch to "Can Penn hit 1000 miles this month?" collective goal

**If Mitigation Fails**: Demonstrate system with simulated data (team logs activities manually under fake user accounts), acknowledge recruitment challenge in presentation, propose future recruitment strategy. Grade likely B-/C+ range.

---

### Risk 3: Fraud/Data Quality Collapse üü° MEDIUM
**Likelihood**: 40%
**Impact**: Undermines platform integrity, demotivates honest users

**Description**: Without API verification (cut in descope), users exaggerate or fabricate activity to win leaderboard. One user logs "100 mile run" daily. Honest users see this, feel cheated, stop participating. Platform becomes competition of who lies best, destroying core value proposition.

**Mitigation Strategies**:
- **Hard limit rules (simple)**: Max 50 miles/day, 6 hours/day activity - reject anything above
- **Community moderation**: "Flag suspicious" button, 3 flags = manual review by team
- **Social accountability**: Require team captains to vouch for members (peer pressure for honesty)
- **Trust scores**: Track submission history, down-weight repeat offenders
- **Leaderboard filtering**: "Verified Only" mode (even if verification is manual spot-checks)
- **Transparency**: Show activity details publicly - "User123 logged 80 miles walking yesterday" is obviously fraudulent

**If Mitigation Fails**: Pivot messaging from "competitive leaderboard" to "personal accountability tool" - emphasize individual streaks and goals over rankings. This removes cheating incentive while preserving core habit-building value.

---

## Decision Guidance

### Recommendation: NEEDS REVISION ‚Üí Can become WEAK GO

**Current State**: 18/30 (Needs Major Revision)
**Post-Revision Potential**: 23-24/30 (Weak GO)

The project has excellent crowdsourcing fundamentals but catastrophic technical overscoping. With mandatory 70% descope, it can become viable.

---

### Most Critical Change Needed

**Cut mobile development entirely and eliminate all API integrations - build web-only manual logging MVP with simple leaderboards for 30-50 Penn students in a single 4-week challenge.**

---

### Predicted Outcome: If No Changes Made

**Week 1-2**: Team installs React Native, fights with iOS simulator, attempts HealthKit integration, discovers 2-week API approval process.

**Week 3**: Panic. Realize mobile is impossible. Emergency pivot to web app. Frantically build basic auth + database.

**Week 4**: All-nighters building manual logging + leaderboard. Recruitment starts too late. Get 8 users (friends/family).

**Week 5**: Polish slides. Demo shows decent UI with sparse data. Presentation focuses on "what it would be" rather than "what it is."

**Demo Day**: Slick presentation of the vision. Live demo shows 3 teams with 8 total users, leaderboard has 12 activities over 2 weeks. Map view is static mockup. Rewards system is "coming soon."

**Grade**: B-/C+ range. Feedback: "Great idea and design, but overscoped led to underdelivery. Should have built less, finished more."

**What Could Have Been**: With proper scoping, could have had 50 real Penn students logging workouts for 3 weeks with active leaderboards, demonstrating real crowdsourcing dynamics and earning A-/B+ grade.

---

### Predicted Outcome: If Changes Are Made

**Revised Scope**: Web app (React + Firebase), manual activity logging, team-based leaderboards, simple point system, Penn-only pilot.

**Week 1**: Working auth + database + basic logging form. Team starts logging own activities.

**Week 2**: Leaderboard display + team assignment. Recruit 30 Penn students from specific club.

**Week 3**: Add streaks + basic QC rules. Users actively logging workouts.

**Week 4**: Bug fixes, user feedback, recruitment push to 50 users.

**Week 5**: Polish, analyze data, prepare demo.

**Demo Day**: Live demo with 40-50 real users, 200+ activities logged over 3 weeks, 4 active teams with competitive leaderboards. Real data showing team dynamics, consistency patterns, quality control working.

**Grade**: B+/A- range. Demonstrates successful crowdsourcing system with real users, though scaled back from initial vision.

---

## Comparison to V1 Analysis

### What Changed

The V2 analysis using the structured rubric provides:

1. **Quantified assessment**: V1 said "D technical feasibility" - V2 scores it 1/5 with total 18/30, clearly placing it in "Needs Major Revision" category

2. **Dimension-by-dimension breakdown**: V1 focused heavily on technical issues - V2 reveals that Incentive Design (5/5) and Crowdsourcing Viability (4/5) are actually excellent, balancing the critique

3. **Concrete improvement pathways**: V1 said "cut mobile, cut APIs" - V2 provides specific Week-by-Week scope, exact QC mechanisms for MVP, detailed recruitment strategies

4. **Risk prioritization**: V2 clearly ranks Technical Impossibility (95% likelihood) > Cold Start (60%) > Fraud (40%), helping team focus mitigation efforts

5. **Nuanced verdict**: V1 said "WEAK GO with mandatory scope reduction" - V2 says "NEEDS REVISION ‚Üí can become WEAK GO" with specific path from 18/30 to 23-24/30

### Additional Insights from Rubric

1. **Incentive Design Excellence**: The rubric's focus on effort/incentive matching revealed this is actually a strength (5/5) - the core task (exercise + log) perfectly matches gamification incentives. V1 acknowledged this ("actually good incentive design") but V2 quantifies why it's best-in-class.

2. **Scope vs Feasibility Distinction**: V1 conflated these - V2 separates them:
   - **Technical Feasibility (1/5)**: Can you build it? No, mobile + APIs is impossible.
   - **Scope Appropriateness (1/5)**: Is it the right amount? No, 10 features is 3x too many.
   - This clarifies that even if team could theoretically build it, they're attempting too much.

3. **Quality Control Sophistication**: V1 called it "sophisticated QC thinking" in passing - V2 scores it 4/5 and explains why the multi-layered approach (API verification + outlier detection + reputation) demonstrates genuine understanding of crowdsourcing principles, even if overengineered for MVP.

4. **Recruitment Partial Credit**: V1 praised "smart local pilot strategy" - V2 scores it 3/5, acknowledging the Penn Challenge approach is good BUT insufficient without pre-commitments and unrealistic on scale (500+ target).

5. **Predicted Outcome Scenarios**: V1 gave one timeline - V2 provides two paths (no changes vs changes made), making the consequences of decisions crystal clear.

6. **Actionable Metrics**: V2 provides testable checkpoints:
   - "Get 30 signups before building" (recruitment viability)
   - "Build core loop in 2 weeks" (technical feasibility)
   - "Feature freeze Week 3" (scope management)

### What Remained Consistent

Both V1 and V2 agree on:
- **Core issue**: Catastrophic technical overscoping
- **Main strength**: Excellent incentive alignment and crowdsourcing principles
- **Critical change**: Cut mobile, cut APIs, focus on web-only manual logging
- **Verdict category**: Needs significant revision but salvageable
- **Root cause**: Team has startup vision, not class project scope

### Rubric Value-Add

The structured rubric forced systematic evaluation of all six dimensions, preventing the analysis from being dominated solely by technical concerns. This revealed:
- 2 dimensions are excellent (Incentive Design, QC)
- 2 dimensions are good (Crowdsourcing Viability, Recruitment)
- 2 dimensions are critical failures (Technical Feasibility, Scope)

This balanced view helps the team understand they have strong foundations to build on - they don't need to rethink the concept, just ruthlessly descope the execution.

The 18/30 score also provides clear benchmark: with recommended changes (web-only, cut APIs, 30 user target, simplified QC), the project could realistically reach 23-24/30 (Weak GO), as:
- Technical Feasibility: 1‚Üí3 (+2)
- Scope Appropriateness: 1‚Üí4 (+3)
- Recruitment Strategy: 3‚Üí4 (+1)
- Quality Control: 4‚Üí3 (-1, simplified)

This shows a viable path forward rather than just listing problems.

---

## Final Recommendation Summary

**DO NOT PROCEED with current scope.**

**DO PROCEED with mandatory revisions**:
1. Web app only (no mobile)
2. Manual logging only (no APIs)
3. Simple leaderboard (no real-time map)
4. 30 user target (not 500+)
5. Basic QC rules (no ML)
6. No rewards redemption

**Timeline for Decision**:
- Next 48 hours: Get 30 signups via Google Form
- If successful: Build descoped MVP
- If unsuccessful: Pivot to different project idea

**Expected Grade Range**:
- Current scope: B-/C+ (overscoped disaster)
- Revised scope: B+/A- (working crowdsourcing system with real users)

**Key Insight**: This is an A+ idea being executed at F scope. With better scoping discipline, it's a solid B+/A- project that demonstrates real crowdsourcing principles.
